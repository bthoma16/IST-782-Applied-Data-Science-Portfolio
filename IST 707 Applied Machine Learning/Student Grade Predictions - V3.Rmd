---
title: "Student Grade Predictions"
output: html_document
date: "2023-05-06"
---

```{r}
#install.packages("tidyverse")
#install.packages("readr")
#install.packages("rpart")
#install.packages("rpart.plot")
#install.packages("caret")
#install.packages("e1071")
#install.packages("randomForest")
#install.packages("arules")
#install.packages("arulesViz")
```

```{r, warning=FALSE}
set.seed(4321)
options(warn=-1)
library(tidyverse)
library(readr)
library(rpart)
library(rpart.plot)
library(caret)
library(e1071)
library(randomForest)
library(arules)
library(arulesViz)
```

```{r}
#Insert the path to the data on your device here: (use / not \)
path <- "C:/Users/Owner/Documents/SU_Q3/IST 707/Project/student-mat.csv"
```

```{r}
student_df <- read.csv(path)
head(student_df)
```
**Data Prep**
```{r}
#Our Target Audience are those who are really struggling, achieving a 55% in the class or less
11/20
```

```{r}
#creating the target variable 
student_df$Target <- ifelse(student_df$G3 <= 11, 1, 0)
student_df$Target <- as.factor(student_df$Target)
```

Now, the variable types require updating to reflect their true nature 
```{r}
str(student_df)
```

The values will be updated before the models are created 


Check for Null values 
```{r}
#The number of rows matches the df even after na.omit() which tells us there is no Null values in the data frame 
student_df %>%
  na.omit() %>%
  nrow()
```


Some outliers but want to not remove them as we are focusing on those who fail 
```{r}
student_df %>%
  ggplot() + 
  aes(x=sex, y=G3) +
  geom_boxplot(fill = "lightgreen") +
  ggtitle("There are two outliers, both students had 0's") +
  coord_flip() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) 
```


**Exploratory Data Analysis** 

```{r}
student_df %>%
  select(c(school, G1, G2, G3)) %>%
  group_by(school) %>%
  summarise_at(.vars = vars(G1, G2, G3),
               .funs = c(mean = "mean")) %>%
  pivot_longer(cols = c('G1_mean', 'G2_mean', 'G3_mean'),
               names_to = 'Grades',
               values_to = 'Average_Grade') %>%
  ggplot() +
  aes(x = Grades, y = Average_Grade, group = school, color = school, alpha = 1) + 
  geom_line(linewidth = 2) +
  geom_point(size = 4) +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + 
  ylab("Average Grades") +
  xlab("Grades for each Period") +
  ggtitle("For Each School, Grades slope downwards") +
  scale_y_continuous(limits = c(0,11)) 
```

```{r}
## students who get zeros increase a significant amount by period 
G1 <- sum(student_df$G1 == 0, na.rm=TRUE)
G2 <- sum(student_df$G2 == 0, na.rm=TRUE)
G3 <- sum(student_df$G3 == 0, na.rm=TRUE)
zero_grades <- data.frame(G3, G2, G1)
zero_grades %>%
  pivot_longer(cols = c(G3, G2,G1),names_to = "Time_of_Year", values_to = "number_of_students") %>%
  ggplot() + 
  aes(x = Time_of_Year, y = number_of_students) + 
  geom_bar(stat = 'identity', fill = "lightgreen")  +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  coord_flip() + 
  xlab("Part of School Year") + 
  ylab("Number of students with a grade of 0") +
  ggtitle("As the School Year goes on, more students start receiving grades of 0")

```


```{r}
student_df %>%
  ggplot() +
  aes(x=Target) +
  geom_histogram(stat = "count", fill = "lightgreen") + 
  ggtitle("A Majority of Students are Failing the Math Course") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  ylab("Number of Students") +
  coord_flip()
```
```{r}
student_df %>%
  group_by(sex) %>%
  summarize(average_final_grade = mean(G3)) %>%
  ggplot() +
  aes(x = sex, y=average_final_grade) + 
  geom_bar(stat = "identity", fill = "lightgreen") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  ggtitle("Females Final Grades are Slightly Below Mens") +
  ylab("Average Final Grade") +
  coord_flip()
```
   
```{r}
student_df %>%
  group_by(school, Target) %>%
  count() %>%
  ggplot() + 
  aes(x=school, y=n, fill = Target) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette = "Accent") +
  coord_flip() + 
   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  ylab("Number of Students") +
  ggtitle("Both schools have around the same proportion of students who fail")
```
```{r}
student_df %>%
  group_by(Fjob, Target) %>%
  count() %>%
  ggplot() + 
  aes(x=Fjob, y=n, fill = Target) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette = "Accent") +
  coord_flip() + 
   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  ylab("Number of Students") +
  ggtitle("Students with Fathers who work in Services struggle with the class")
```
```{r}
student_df %>%
  group_by(Dalc, Target) %>%
  count() %>%
  ggplot() + 
  aes(x=Dalc, y=n, fill = Target) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette = "Accent") +
  coord_flip() + 
   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
  ylab("Number of Students") +
  ggtitle("Those with a level 2 (moderate) alcohol consumption tend to fail more")
```


```{r}
student_df %>%
  group_by(activities, Target) %>%
  count() %>%
  ggplot() + 
  aes(x=activities, y=n, fill = Target) +
  geom_bar(stat = "identity") + 
  scale_fill_brewer(palette = "Accent") + 
 theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())  + 
  coord_flip()
```


```{r}
student_df %>%
  group_by(school, famsize) %>%
  count() %>%
  ggplot() +
  aes(fill = famsize, y = n, x=school) +
  geom_bar(position = "fill", stat="identity")  + 
 # ggtitle("School B has majority of students who have Completed or on Schedule to Complete") +
  coord_flip() +
   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.title = element_text(size = 12)) +
  scale_fill_brewer(palette = "Accent")
  
```
```{r}
student_df %>%
  group_by(paid, Target) %>%
  count() %>%
  ggplot() +
  aes(fill = Target, y = n, x=paid) +
  geom_bar(position = "fill", stat="identity")  + 
  ggtitle("The proportion shows that the paid extra classes don't seem to provide help") +
  coord_flip() +
   theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), plot.title = element_text(size = 12)) +
  scale_fill_brewer(palette = "Accent")
  
```






```{r}
student_df <- student_df %>%
  select(-c(G3, G2, G1, paid))
```


**Modeling**

Updating variable types for model performance 
```{r}
student_df <- student_df %>% 
  mutate(school = as.factor(school)) %>%
  mutate(sex = as.factor(sex)) %>%
  mutate(address = as.factor(address)) %>%
  mutate(famsize = as.factor(famsize)) %>%
  mutate(Pstatus = as.factor(Pstatus)) %>%
  mutate(Medu = factor(Medu, ordered = TRUE, levels = c(0, 1, 2, 3, 4))) %>%
  mutate(Fedu = factor(Fedu, ordered = TRUE, levels = c(0, 1, 2, 3, 4))) %>%
  mutate(schoolsup = as.factor(schoolsup)) %>%
  mutate(famsup = as.factor(famsup)) %>%
  mutate(activities = as.factor(activities)) %>%
  mutate(nursery = as.factor(nursery)) %>%
  mutate(higher = as.factor(higher)) %>%
  mutate(internet = as.factor(internet))  %>%
  mutate(romantic = as.factor(romantic)) %>%
  mutate(famrel = factor(famrel, ordered = TRUE, levels = c(0, 1, 2, 3, 4, 5))) %>%
  mutate(freetime = factor(freetime, ordered = TRUE, levels = c(0, 1, 2, 3, 4, 5))) %>%
  mutate(goout = factor(goout, ordered = TRUE, levels = c(0, 1, 2, 3, 4, 5))) %>%
  mutate(Dalc = factor(Dalc, ordered = TRUE, levels = c(0, 1, 2, 3, 4, 5))) %>%
  mutate(Walc = factor(Walc, ordered = TRUE, levels = c(0, 1, 2, 3, 4, 5))) %>%
  mutate(health = factor(health, ordered = TRUE, levels = c(0, 1, 2, 3, 4, 5))) 
```




```{r}
sample <- sample(c(TRUE, FALSE), nrow(student_df), replace=TRUE, prob=c(0.7,0.3))
train_student  <- student_df[sample, ]
test_student   <- student_df[!sample, ]
```



*Decision Tree* 
*Unpruned*
```{r}
tree_model_unprune <- rpart(Target ~., data = train_student
, method = 'class'
, model = T
)
rsq.rpart(tree_model_unprune)
```
```{r}
rpart.plot(tree_model_unprune)
```
```{r}
preds_unprune <- predict(tree_model_unprune, test_student, type="class")
confusionMatrix(data = preds_unprune, reference = test_student$Target)
```
```{r}
df <- data.frame(imp = tree_model_unprune$variable.importance)

 df %>%
  tibble::rownames_to_column() %>%
  dplyr::rename("variable" = rowname) %>%
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable)) %>%
  ggplot() +
  geom_col(aes(x=variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()
```



*Pruned-1*
```{r}
#last model to remove to the prune to 0 to remove some potential stems if above 0 and adding some cross validation to the model and focusing on percentage rather than missing values
tree_model_prune_1 <- rpart(Target ~., data = train_student
, method = 'class'
,control = rpart.control(minsplit = 6,  maxdepth = 4)
, model = T
)
rsq.rpart(tree_model_prune_1)
```


```{r}
rpart.plot(tree_model_prune_1)
```

```{r}
preds_prune_1 <- predict(tree_model_prune_1, test_student, type="class")
confusionMatrix(data = preds_prune_1, reference = test_student$Target)
```

*Pruned-2*
```{r}
train_student_1 <- train_student %>% select(c("failures", "Target", "freetime", "Fedu", "Dalc",   "Walc",  "schoolsup", "absences", "age", "famrel", "famsize"))

test_student_1 <- train_student %>% select(c("failures", "Target", "freetime", "Fedu", "Dalc",   "Walc",  "schoolsup", "absences", "age", "famrel", "famsize"))
```

```{r}
#last model to remove to the prune to 0 to remove some potential stems if above 0 and adding some cross validation to the model and focusing on percentage rather than missing values
tree_model_prune_2 <- rpart(Target ~., data = train_student_1
, method = 'class'
, control = rpart.control(minbucket = 10,  maxcompete = 2, xval = 10)
, model = T
)
rsq.rpart(tree_model_prune_2)
```


```{r}
rpart.plot(tree_model_prune_2)
```


```{r}
preds_prune_2 <- predict(tree_model_prune_2, test_student_1, type="class")
confusionMatrix(data = preds_prune_2, reference = test_student_1$Target)
```

```{r}
df <- data.frame(imp = tree_model_prune_2$variable.importance)

 df %>%
  tibble::rownames_to_column() %>%
  dplyr::rename("variable" = rowname) %>%
  dplyr::arrange(imp) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable)) %>%
  ggplot() +
  geom_col(aes(x=variable, y = imp),
           col = "black", show.legend = F) +
  coord_flip() +
  scale_fill_grey() +
  theme_bw()
```

*SVM* 
```{r}
# SVM model 1 - Tuning with a polynomial kernel

polynomialModel1 <- svm(Target ~ ., data = train_student_1, kernel = "polynomial", cost = 0.15, scale = FALSE)

# Prediction and accuracy


polymodel1Pred <- predict(polynomialModel1, newdata = test_student_1)

polymodel1CM <- confusionMatrix(table(polymodel1Pred, test_student_1$Target))
polymodel1CM

# The accuracy of the model is 60.5% and the misclassification rate is 39.5%
```


```{r}
# SVM model 2 - Tuning with a radial kernel

radialModel2 <- svm(Target ~ ., data = train_student_1, kernel = "radial", cost = 0.95, scale = FALSE)

# Prediction and accuracy

radmodel2Pred <- predict(radialModel2, newdata = test_student_1)

radmodel2CM <- confusionMatrix(table(radmodel2Pred, test_student_1$Target))
radmodel2CM

# The accuracy of the model is 63.87% and the misclassification rate is 36.13%
```


```{r}
# SVM model 3 - Tuning with a sigmoid kernel

sigmoidModel3 <- svm(Target ~ ., data = train_student, kernel = "sigmoid", cost = 0.95, scale = FALSE)

# Prediction and accuracy

sigmodel3Pred <- predict(sigmoidModel3, newdata = test_student)

sigmodel3CM <- confusionMatrix(table(sigmodel3Pred, test_student$Target))
sigmodel3CM

# The accuracy of the model is 61.34% and the misclassification rate is 38.66%
```




*Random Forest* 
```{r}
# Random Forest model 1 - default parameters

rfModel1 <- randomForest(Target ~ ., data = train_student_1, importance = TRUE)

# Prediction and accuracy

rfmodel1Pred <- predict(rfModel1, newdata = test_student_1)

rfmodel1CM <- confusionMatrix(table(rfmodel1Pred, test_student_1$Target))
rfmodel1CM

# The accuracy of the model is 59.66%
```




```{r}
# Identifying the most appropriate mtry for model 3

# Pre-processing
trainwithoutTarget <- subset(train_student, select = -c(Target))

trainwithTarget <- train_student$Target

testwithoutTarget <- subset(test_student, select = -c(Target))

testwithTarget <- test_student$Target


a = c()
i = 5
for (i in 3:10) {
  idealrfModel <- randomForest(Target ~ ., data = train_student, ntree = 500, mtry = i, importance = TRUE)
  idealrfmodelPred <- predict(idealrfModel, testwithoutTarget, type = "class")
  a[i-2] = mean(idealrfmodelPred == testwithTarget)
}

plot(a*100, type='s', las=1, ylab = "Percent Accuracy", xlab = "mtry value", main = "Percent accuracy Vs. Mtry value")

# The accuracy of mtry increased from 1 to in between 2 and 5. The next increase was in between 7 and 8. Therefore, a mtry value of 4 will be used
```


```{r}
# Random Forest model 3 - Tuning mtry = 4


rfModel3 <- randomForest(Target ~ ., data = train_student_1, ntree = 500, mtry = 4, importance = TRUE)

# Prediction and accuracy


rfmodel3Pred <- predict(rfModel3, newdata = test_student_1)

rfmodel3CM <- confusionMatrix(table(rfmodel3Pred, test_student_1$Target))
rfmodel3CM

# The accuracy of the model is 61.34%
```

*Logistic Regression* 
```{r}
log_1 <- glm(Target~. , family= binomial, data=student_df)
summary(log_1)
```

```{r}
preds_log_1 <- predict(log_1, test_student, type="response")
preds_log_1.classes <- ifelse(preds_log_1 > 0.6, 1, 0)
result_1 <- data.frame(preds_log_1.classes, test_student$Target)
result_1$Correct <- ifelse(result_1$preds_log_1.classes == result_1$test_student.Target, 1, 0)
sum(result_1$Correct)/nrow(result_1)
```



```{r}
log_2 <- glm(Target~ sex + Fjob + failures + schoolsup + freetime + goout, family= binomial, data=student_df)
summary(log_2)
```

```{r}
log_2 <- glm(Target~., family= binomial, data=train_student_1)
summary(log_2)
```

```{r}
preds_log_2 <- predict(log_2, test_student_1, type="response")
preds_log_2.classes <- ifelse(preds_log_2 > 0.6, 1, 0)
mean(preds_log_2.classes == test_student_1$Target)
```



```{r}
log_3 <- glm(Target~ failures + schoolsup, family= binomial, data=student_df)
summary(log_3)
```


```{r}
preds_log_3 <- predict(log_3, test_student, type="response")
preds_log_3.classes <- ifelse(preds_log_3 > 0.6, 1, 0)
mean(preds_log_3.classes == test_student$Target)
```



*Naive Bayes* 
```{r}
naivebayes_model <-naiveBayes(Target~., data=train_student, na.action = na.pass)
```

```{r}
nb_Pred <- predict(naivebayes_model, test_student)
confusionMatrix(data = nb_Pred, reference = test_student$Target)
```
Sensitivity (true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.
Specificity (true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.



***k-NN***

```{r}
trainControl <- trainControl(method="repeatedcv", number=10, repeats=3)
fit.knn <- train(Target ~., data = train_student, method = 'knn', metric = 'Accuracy', trControl = trainControl)
knn.k1 <- fit.knn$bestTune
print(fit.knn)
``` 

```{r}
plot(fit.knn)
```


```{r}
grid <- expand.grid(.k=seq(1,20,by=1))
fit.knn <- train(Target~., data=train_student_1, method='knn', metric = 'Accuracy', tuneGrid=grid, trControl=trainControl)
knn.k2 <- fit.knn$bestTune
print(fit.knn)
```

Accuracy is highest when k=4 

```{r}
plot(fit.knn)
```


```{r}
prediction <- predict(fit.knn, newdata = test_student_1)
cf <- confusionMatrix(prediction, test_student_1$Target)
print(cf)
```


*Association Rule Mining*
```{r}
transactions <- as(student_df, 'transactions')
itemFrequencyPlot(transactions, topN=20, type='absolute')
```


```{r}
rules <- apriori(transactions, parameter = list(supp = 0.03, conf = 0.8), appearance= list(default = 'lhs', rhs = 'Target=1'))
rules <- sort(rules, decreasing = TRUE, by = 'lift')
```


```{r}
inspect(rules[1:40])
```
